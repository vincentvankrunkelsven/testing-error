---
title       : Confidence Intervals
description : Big ideas of confidence intervals (estimation instead of testing).  We'll switch to one proportion for ease in interpretation.  Focus on foundational ideas, not specific details

--- type:VideoExercise lang:r xp:50 skills:1 key:be96c0f023
## Confidence Intervals

Introduction to the idea of confidence intervals.  Focus on the idea of wanting to know the parameter.  Example about the presidential election.

*** =video_link
//player.vimeo.com/video/154783078

*** =video_hls
//videos.datacamp.com/transcoded/000_placeholders/v1/hls-temp.master.m3u8

--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:3f2fd63176
## What is the parameter?

Using the current presidential election - 6 months prior!!  We get to talk about so many good aspects of understanding parameters.

*** =instructions
- option 1
- option 2
- option 3

*** =hint
hint

*** =pre_exercise_code
```{r}
# pec
```

*** =sct
```{r}
test_mc(2, feedback_msgs = c('wrong', 'right', 'wrong')) # if 2 is the correct option.
```

--- type:NormalExercise lang:r xp:100 skills:1 key:ca3fcf9628
## Re-sampling from a sample - same n

Notice that the variability in p-hat is approximately the same whether we sample from the population or re-sample from the sample

What if we asked 30 people, and (1) the true proportion in the population was 0.4, and (2) 10 people said they supported the candidate?

*** =instructions
- instruction 1
- instruction 2

something here about how they should change the seed to see that it'll work regardless of the randomness.  However, set the seed so that they get the answer right.

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(dplyr)
library(ggplot2)
library(oilabs)
```

*** =sample_code
```{r}
# sample code


# first think about the sampling distribution of p-hat
set.seed(47)
polls <- data.frame(prop_yes = rbinom(1000, 30, .4) / 30)

polls %>% 
    summarize(ave_prop = mean(prop_yes),
              sd_prop = sd(prop_yes),
              q025_prop = quantile(prop_yes, p=.025),
              q975_prop = quantile(prop_yes, p=.975))


ggplot(polls, aes(prop_yes)) + geom_histogram(binwidth=0.01)


#then think about the sampling distribution created by resampling from one sample


# i'm stuggling with teaching ideas vs. teaching R.  
# i've written it simply in R.  however, i'm sure rbinom(30,1,.4) vs. rbinom(100,30,.4)
# will be very confusing to students

# additionally... rep_sample_n didn't work when onepoll was a data frame (it had to be a tbl)

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))

onepoll %>%
  summarize(mean(votes))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

onepoll_boot %>% 
    summarize(ave_prop = mean(prop_yes_boot),
              sd_prop = sd(prop_yes_boot),
              q025_prop = quantile(prop_yes_boot, p=.025),
              q975_prop = quantile(prop_yes_boot, p=.975))


ggplot(onepoll_boot, aes(prop_yes_boot)) + geom_histogram(binwidth=0.01)


```

*** =solution
```{r}
# solution code


# first think about the sampling distribution of p-hat
set.seed(47)
polls <- data.frame(prop_yes = rbinom(1000, 30, .4) / 30)

polls %>% 
    summarize(ave_prop = mean(prop_yes),
              sd_prop = sd(prop_yes),
              q025_prop = quantile(prop_yes, p=.025),
              q975_prop = quantile(prop_yes, p=.975))


ggplot(polls, aes(prop_yes)) + geom_histogram(binwidth=0.01)


#then think about the sampling distribution created by resampling from one sample


# i'm stuggling with teaching ideas vs. teaching R.  
# i've written it simply in R.  however, i'm sure rbinom(30,1,.4) vs. rbinom(100,30,.4)
# will be very confusing to students

# additionally... rep_sample_n didn't work when onepoll was a data frame (it had to be a tbl)

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))

onepoll %>%
  summarize(mean(votes))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

onepoll_boot %>% 
    summarize(ave_prop = mean(prop_yes_boot),
              sd_prop = sd(prop_yes_boot),
              q025_prop = quantile(prop_yes_boot, p=.025),
              q975_prop = quantile(prop_yes_boot, p=.975))


ggplot(onepoll_boot, aes(prop_yes_boot)) + geom_histogram(binwidth=0.01)
```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:0919244cc1
## Re-sampling from a sample - big n

Notice that the variability in p-hat is non-existent

*** =instructions
- instruction 1
- instruction 2

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(tidyr)

set.seed(47)
polls <- data.frame(prop_yes = rbinom(1000, 30, .4) / 30)

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))

```

*** =sample_code
```{r}
# sample code
polls %>% 
    summarize(ave_prop = mean(prop_yes),
              sd_prop = sd(prop_yes),
              q025_prop = quantile(prop_yes, p=.025),
              q975_prop = quantile(prop_yes, p=.975))
onepoll %>%
  summarize(mean(votes))

onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(300, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

set.seed(47)
onepoll_boot %>% 
    summarize(ave_prop = mean(prop_yes_boot),
              sd_prop = sd(prop_yes_boot),
              q025_prop = quantile(prop_yes_boot, p=.025),
              q975_prop = quantile(prop_yes_boot, p=.975))


ggplot(onepoll_boot, aes(prop_yes_boot)) + geom_histogram(binwidth=0.01)

```

*** =solution
```{r}
# solution code
polls %>% 
    summarize(ave_prop = mean(prop_yes),
              sd_prop = sd(prop_yes),
              q025_prop = quantile(prop_yes, p=.025),
              q975_prop = quantile(prop_yes, p=.975))
onepoll %>%
  summarize(mean(votes))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(300, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

onepoll_boot %>% 
    summarize(ave_prop = mean(prop_yes_boot),
              sd_prop = sd(prop_yes_boot),
              q025_prop = quantile(prop_yes_boot, p=.025),
              q975_prop = quantile(prop_yes_boot, p=.975))


ggplot(onepoll_boot, aes(prop_yes_boot)) + geom_histogram(binwidth=0.01)
```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:a74847b5eb
## Re-sampling from a sample - small n

Notice that we get variability in p-hat that seems reasonable

*** =instructions
- instruction 1
- instruction 2

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(tidyr)

set.seed(47)
polls <- data.frame(prop_yes = rbinom(1000, 30, .4) / 30)

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))
```

*** =sample_code
```{r}
# sample code

polls %>% 
    summarize(ave_prop = mean(prop_yes),
              sd_prop = sd(prop_yes),
              q025_prop = quantile(prop_yes, p=.025),
              q975_prop = quantile(prop_yes, p=.975))
onepoll %>%
  summarize(mean(votes))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(3, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

onepoll_boot %>% 
    summarize(ave_prop = mean(prop_yes_boot),
              sd_prop = sd(prop_yes_boot),
              q025_prop = quantile(prop_yes_boot, p=.025),
              q975_prop = quantile(prop_yes_boot, p=.975))


ggplot(onepoll_boot, aes(prop_yes_boot)) + geom_histogram(binwidth=0.01)
```

*** =solution
```{r}
# solution code
polls %>% 
    summarize(ave_prop = mean(prop_yes),
              sd_prop = sd(prop_yes),
              q025_prop = quantile(prop_yes, p=.025),
              q975_prop = quantile(prop_yes, p=.975))
onepoll %>%
  summarize(mean(votes))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(3, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

onepoll_boot %>% 
    summarize(ave_prop = mean(prop_yes_boot),
              sd_prop = sd(prop_yes_boot),
              q025_prop = quantile(prop_yes_boot, p=.025),
              q975_prop = quantile(prop_yes_boot, p=.975))


ggplot(onepoll_boot, aes(prop_yes_boot)) + geom_histogram(binwidth=0.01)
```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:VideoExercise lang:r xp:50 skills:1 key:0b1d4f227a
## Variability in p-hat

Discuss their normal exercises.  Maybe also talk about SE(phat) = sqrt(p(1-p)/n)... or at least that we know a formula which works well a lot of the time. Then the big important point is that true p is as far from p-hat as p-hat is from true p.  Measured by exactly the variability we just saw.  Also, show Empirical Rule.

*** =video_link
//player.vimeo.com/video/154783078

*** =video_hls
//videos.datacamp.com/transcoded/000_placeholders/v1/hls-temp.master.m3u8

--- type:NormalExercise lang:r xp:100 skills:1 key:13c2b4af1c
## Empirical Rule (1)

Notice that approximately the 95% of the p-hat values are within 2 SE

*** =instructions
- instruction 1
- instruction 2

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(tidyr)

set.seed(47)
polls <- data.frame(prop_yes = rbinom(1000, 30, .4) / 30)

```

*** =sample_code
```{r}
# sample code
polls %>%
  summarize(sum( polls < 0.4 - 2*sd(prop_yes)),
            sum( polls > 0.4 + 2*sd(prop_yes)))
```

*** =solution
```{r}
# solution code
polls %>%
  summarize(sum( polls < 0.4 - 2*sd(prop_yes)),
            sum( polls > 0.4 + 2*sd(prop_yes)))
```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:071683bccd
## Empirical Rule (2)

So, 95% of p-hat values are within 2SE of the true value!

*** =instructions
- instruction 1
- instruction 2

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(tidyr)

set.seed(47)
polls <- data.frame(prop_yes = rbinom(1000, 30, .4) / 30)

```

*** =sample_code
```{r}
# sample code
polls %>%
  summarize(sum( 0.4 < polls - 2*sd(prop_yes)),
            sum( 0.4 > polls + 2*sd(prop_yes)))

```

*** =solution
```{r}
# solution code
```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:ffec7ad763
## t-Confidence Interval

Have them actually make the t-CI

*** =instructions
- instruction 1
- instruction 2

we only got one p-hat.  and we used the bootstrapping technique to find the SE.  so if we go from our p-hat plus or minus 2SE, we hope we are one of the approximately 95% of intervals that capture the true value.

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(tidyr)

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

```

*** =sample_code
```{r}
# sample code

# I need help here.  How do I take the average from the original data (onepoll) and plus or minus the SE from the bootstrap data?  That is, how would it be best to use dplyr to work with both dataframes at the same time?  And what about $?  Do they know about $?

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper
```

*** =solution
```{r}
# solution code
lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper
```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:VideoExercise lang:r xp:50 skills:1 key:d995c94a95
## Interpreting CIs and technical conditions

Discuss the interval they made and what it says about the parameter.  Then discus how the process works for any statistic/parameter pair when the technical conditions are met.  State technical conditions and refer to later in the course (averages) for more details.

*** =video_link
//player.vimeo.com/video/154783078

*** =video_hls
//videos.datacamp.com/transcoded/000_placeholders/v1/hls-temp.master.m3u8

--- type:NormalExercise lang:r xp:100 skills:1 key:db1cdf3564
## Sample size effects CI

Repeat process of finding CI for smaller / larger sample sizes

*** =instructions
- instruction 1
- instruction 2

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(dplyr)
set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper

```

*** =sample_code
```{r}
# sample code
set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(300, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(300, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(3, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(3, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper

```

*** =solution
```{r}
# solution code
set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(300, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(300, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(3, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(3, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper

```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:c796bc422f
## Sample proportion value effects CI

Repeate process of finding CI for different values of the sample proportion (not much change here!)

*** =instructions
- instruction 1
- instruction 2

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec
library(dplyr)
set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper
```

*** =sample_code
```{r}
# sample code
set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .8)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper

```

*** =solution
```{r}
# solution code
set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .8)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 

lower <- mean(onepoll$votes) - 2*sd(onepoll_boot$prop_yes_boot)
upper <- mean(onepoll$votes) + 2*sd(onepoll_boot$prop_yes_boot)

lower
upper
```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:d57fc5f3bb
## Percentile Interval - changing the percents

Repeate process of finding CI for different values of the confidence level

*** =instructions
- instruction 1
- instruction 2

*** =hint
hint comes here

*** =pre_exercise_code
```{r}
# pec

set.seed(47)
onepoll <- as.tbl(data.frame(votes = rbinom(30, 1, .4)))

set.seed(47)
onepoll_boot <- onepoll %>%
  oilabs::rep_sample_n(30, replace=TRUE, reps=10000) %>%
  summarize(prop_yes_boot = mean(votes)) 
```

*** =sample_code
```{r}
# sample code
onepoll_boot %>% 
  summarize(q025_prop = quantile(prop_yes_boot, p=.025),
            q975_prop = quantile(prop_yes_boot, p=.975))

onepoll_boot %>% 
  summarize(q005_prop = quantile(prop_yes_boot, p=.005),
            q995_prop = quantile(prop_yes_boot, p=.995))

onepoll_boot %>% 
  summarize(q05_prop = quantile(prop_yes_boot, p=.05),
            q95_prop = quantile(prop_yes_boot, p=.95))

```

*** =solution
```{r}
# solution code
onepoll_boot %>% 
  summarize(q025_prop = quantile(prop_yes_boot, p=.025),
            q975_prop = quantile(prop_yes_boot, p=.975))

onepoll_boot %>% 
  summarize(q005_prop = quantile(prop_yes_boot, p=.005),
            q995_prop = quantile(prop_yes_boot, p=.995))

onepoll_boot %>% 
  summarize(q05_prop = quantile(prop_yes_boot, p=.05),
            q95_prop = quantile(prop_yes_boot, p=.95))

```

*** =sct
```{r}
success_msg("Great work!")
```

--- type:VideoExercise lang:r xp:50 skills:1 key:9f579ff31d
## Summary of Statistical Inference / Estimation

Summary of Statistical Inference / Estimation

*** =video_link
//player.vimeo.com/video/154783078

*** =video_hls
//videos.datacamp.com/transcoded/000_placeholders/v1/hls-temp.master.m3u8

